config_path: null
output_dir: null
dataset_path: data/test
seed: 42
batch_size: 2
num_workers: 10
prefetch_factor: 20
max_new_tokens: 1024
generation_config:
  do_sample: false
  num_beams: 2
  repetition_penalty: 1.5
  encoder_repetition_penalty: 1.5
  no_repeat_ngram_size: 4
use_regex: true
training_dir: outputs/UseProccessed_3Enc/1223_103055
ckpt_path: outputs/UseProccessed_3Enc/1223_103055/checkpoint
model_config:
  seed: 42
  debug: false
  profile: false
  liger_kernel: true
  wandb: true
  project_name: null
  run_name: null
  finetune_language: true
  resume: outputs/NoLora_UseProccessed_3Enc/1223_043535/checkpoint/
  resume_tag: '1'
  model:
    model_id: llava-hf/llava-1.5-7b-hf
    encoder_id: facebook/dinov2-large
    only_clip: true
    use_clip: false
    interpolation_mode: bilinear
    share_vit: false
    use_processed: true
    use_depth: true
    depth_model_id: depth-anything/Depth-Anything-V2-Small-hf
    use_segmentation: true
    segmentation_model_id: shi-labs/oneformer_ade20k_dinat_large
    fuser_id: gemini
    conditional_fuser: true
    condition_dropout: 0.3
    no_lora_but_FF_prefix:
      - multi_modal_projector
      - fuser
      - vision_tower.AdaLNZero
      - auxiliary_projectors
    vision_feature_select_strategy: full
    gradient_checkpointing: true
    lora_config:
      r: 32
      lora_alpha: 32
      target_modules:
        - q_proj
        - v_proj
      exclude_modules: vision_tower.*
      lora_dropout: 0.1
      use_dora: true
      bias: none
      task_type: CAUSAL_LM
  dataset:
    dataset_path: data
    num_workers: 20
    prefetch_factor: 40
  deepspeed:
    epochs: 2
    learning_rate: 3.0e-05
    batch_size: 2
    accumulation_steps: 4
    config:
      gradient_clipping: 1.0
      bf16:
        enabled: true
      flops_profiler:
        enabled: false
        detailed: false
