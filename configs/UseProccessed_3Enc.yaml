seed: 42
debug: false
profile: false
liger_kernel: true
wandb: true
project_name: null
run_name: null
finetune_language: true
resume: outputs/NoLora_UseProccessed_3Enc/1223_043535/checkpoint/
resume_tag: 1
model:
  model_id: llava-hf/llava-1.5-7b-hf
  encoder_id: facebook/dinov2-large
  share_vit: false
  use_processed: true
  use_depth: true
  depth_model_id: depth-anything/Depth-Anything-V2-Small-hf
  use_segmentation: true
  segmentation_model_id: shi-labs/oneformer_ade20k_dinat_large
  fuser_id: gemini
  conditional_fuser: true
  condition_dropout: 0.3
  no_lora_but_FF_prefix:
  - multi_modal_projector
  - fuser
  - vision_tower.AdaLNZero
  - auxiliary_projectors
  vision_feature_select_strategy: full
  gradient_checkpointing: true
  lora_config:
    r: 32
    lora_alpha: 32
    target_modules:
    - q_proj
    - v_proj
    - multi_modal_projector.linear_1
    - multi_modal_projector.linear_2
    exclude_modules: vision_tower.*
    lora_dropout: 0.1
    use_dora: true
    bias: none
    task_type: CAUSAL_LM
dataset:
  dataset_path: data
  num_workers: 20
  prefetch_factor: 40
deepspeed:
  epochs: 2
  learning_rate: 3.0e-05
  batch_size: 2
  accumulation_steps: 4
  config:
    gradient_clipping: 1.0
    bf16:
      enabled: true
    flops_profiler:
      enabled: false
      detailed: false
    zero_optimization:
      stage: 2
      offload_optimizer:
        device: cpu
        pin_memory: true
      allgather_partitions: true
      allgather_bucket_size: 500000000.0
      overlap_comm: true
      reduce_scatter: true
      reduce_bucket_size: 500000000.0
      contiguous_gradients: true
      round_robin_gradients: true
